{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECS708P_Lab04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOV3srOuboM9"
      },
      "source": [
        "# Lab 4: Exploring overfitting, validation and regularisation in regression\n",
        "\n",
        "Our main goal in machine learning is to create models that will perform well **during deployment**. In contrast to other engineering and scientific approches, in machine learning we **lack a perfect despription** of the target population and all we can do is **sample data** from the population. We use data in different **tasks**: to build our models (**train**), to assess their deployment performance (**test**) and to select a suitable family of models for training (**validation**).\n",
        "\n",
        "In Lab 3, we obtained the least square solution of four families of polynomial models, namely polynomials of degree 1 (linear), 2 (quadratic), 3 (cubic) and 4. Least squares is an exact method and provides the **optimal model** defined by the **empirical error surface**. As you know, the optimal model defined by the empirical error surface is in general different from the optimal model defined by the **true error surface**, which is the model that we are actually looking for. This distinction is important, as our models could perform very well during training, but very poorly during deployment. In such cases, we talk about **overfitting**.\n",
        "\n",
        "Overfitting occurs in scenarios where we have **small dataset** and **complex models** and can be detected by comparing the training performance and the deployment performance. Overfitting can be avoided by having large enough datasets and by controlling the complexity of our models. We can identify the right complexity of our models using **validation** approaches. **Regularisation** methods can also be used to control the flexibility of our models. \n",
        "\n",
        "In this lab we explore overfitting, validation and regularisation in a regression problem. Make sure you revise the contents of weeks 2 and 3, as this lab will illustrate many of the concepts that we have discussed during our lectures and will help you to develop a strong intuition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGGlz1M3iSoU"
      },
      "source": [
        "# A simple regression problem\n",
        "\n",
        "In this lab, we will explore the same simple dataset that we used in Lab 3. This dataset consists of 10 samples described by one predictor `x` and one label `y`. Let's define and plot the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAq7RN1Lbah_"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.set_printoptions(precision=3)\n",
        "\n",
        "xTrain = np.array([0.3000, -0.7700, 0.9000, -0.0400, 0.7400, -0.5800, -0.9200, -0.2100, -0.5400, 0.6800])\n",
        "yTrain = np.array([1.1492,  0.3582, 1.9013,  0.9487, 1.3096,  0.9646,  0.1079,  1.1262,  0.6131, 1.0951])\n",
        "\n",
        "plt.plot(xTrain, yTrain, 'o', label=\"Training Data\")\n",
        "plt.xlabel(\"x (attribute)\", fontsize=18)\n",
        "plt.ylabel(\"y (label)\", fontsize=18)\n",
        "plt.xlim(-1,1)\n",
        "plt.ylim(-0.5,2.5)\n",
        "plt.grid(alpha=0.2)\n",
        "plt.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl2yGUrijdbe"
      },
      "source": [
        "Our next step will be to obtain the **least squares linear** fit. In this lab we will take advantage of the implementation of least squares provided by NumPy (`polyfit`) instead of implementing least squares ourselves. You can find more information about `polyfit` [here](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_C-Tug1kaXb"
      },
      "source": [
        "order = 1\n",
        "w = np.polyfit(xTrain, yTrain, order)\n",
        "print(\"The coefficients of the least squares linear solution are: \", w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGLUZOdKnW4C"
      },
      "source": [
        "These coefficients should be the same as the ones we obtained in Lab 3 with our own implementation of least squares. \n",
        "\n",
        "NumPy also provides `poli1d`, a library class that encapsulates operations involving polynomials. Using `poli1d` is very simple and is described [here](https://numpy.org/doc/stable/reference/generated/numpy.poly1d.html). Let's create a *NumPy polynomial object* defined by the coefficients returned by `polyfit` and print it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBg4ybRmoIYE"
      },
      "source": [
        "p = np.poly1d(w)\n",
        "print(\"The mathematical expression of the fitted linear model is : \", p)\n",
        "print(\"The coefficients of the model are : \", p.c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70hyGRaGX3QC"
      },
      "source": [
        "Once the linear solution `p` has been created, making a prediction is easy. We just need to pass the predictor `x` to the polynomial as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SH7URSyYIG_"
      },
      "source": [
        "x = 0\n",
        "yPred = p(x)\n",
        "print(\"Given a predictor x = \", x, \" the predicted label is y = \", yPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3dCf8Aapsql"
      },
      "source": [
        "You can try several values of $x$ and obtained the predicted label. \n",
        "\n",
        "Let's plot the least squares linear model together with the training dataset: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1Gr65N4pF9S"
      },
      "source": [
        "x_LS = np.linspace(-1,1,100).T\n",
        "y_LS = p(x_LS)\n",
        "\n",
        "plt.plot(xTrain, yTrain, 'o', label=\"Training Data\")\n",
        "plt.plot(x_LS, y_LS, label=\"Linear prediction\")\n",
        "plt.xlabel(\"x (attribute)\", fontsize=18)\n",
        "plt.ylabel(\"y (label)\", fontsize=18)\n",
        "plt.xlim(-1,1)\n",
        "plt.ylim(-0.5,2.5)\n",
        "plt.grid(alpha=0.2)\n",
        "plt.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYgQpJqOYxYf"
      },
      "source": [
        "What is the quality of this linear model? Least squares uses the mean squared error (MSE) as quality metric. Remember that the MSE is defined as \n",
        "\n",
        "$E_{MSE} = \\frac{1}{N}\\sum_{i=1}^{N}{e_i^2}=\\frac{1}{N}\\sum_{i=1}^{N}{(y_i-\\hat{y}_i)^2}$ \n",
        "\n",
        "where $N$ is the number of samples, $y_i$ is the true label and $\\hat{y}_i$ is the predicted label. Let's show step by step how to obtain the training MSE of the fitted linear model:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyswmOTxpqZ5"
      },
      "source": [
        "yPredTrain = p(xTrain)\n",
        "error = yTrain - yPredTrain\n",
        "errorSq = error**2\n",
        "MSETrain = np.sum(errorSq)/10\n",
        "\n",
        "print(\"The predictors are \", xTrain)\n",
        "\n",
        "print(\"\\nThe true labels are \", yTrain)\n",
        "\n",
        "print(\"\\nThe predicted labels s are \", yPredTrain)\n",
        "\n",
        "print(\"\\nThe errors are \", error)\n",
        "\n",
        "print(\"\\nThe squared errors are \", errorSq)\n",
        "\n",
        "print(\"\\nThe training MSE is \", MSETrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbeYdTANqqmA"
      },
      "source": [
        "Have a look at the graph showing the training data and the least squares linear model. Make sure that the individual errors that you have obtained make sense to you before moving ahead. For instance, check those samples where the error is close to zero or identify the samples that have a negative value.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws5hl_QRqzTP"
      },
      "source": [
        "# Complexity and training MSE\n",
        "\n",
        "\n",
        "As we increase the order of a polynomial model, its flexibility increases. Flexible models can produce complex shapes and will therefore be able to reproduce complex patterns. Because of this, flexible models will always make **better predictions** during **training**. There is a risk, as you know. Our training dataset is the result of a **pattern** (which we want to discover) and **irrelevant details** (or noise, which we want to ignore). During optimisation we ask our models to make the best predictions that they can, however this prediction includes both pattern and irrelevant details. The higher the complexity of the model, the higher the risk to learn those irrelevant details, and this will result in a poorer deployment performance. \n",
        "\n",
        "In this section, we will fit several polynomial models of degrees 1 (linear) to 9 to our dataset. We will also obtain the training MSE of each model and the sum of squares of their coefficients (SSC), defined as:\n",
        "\n",
        "$SSC = \\sum_{d=0}^{D}{w_d^2}$\n",
        "\n",
        "where $D$ is the degree of the polynomial and $w_d$ are the coefficients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIZK_CPHf8zI"
      },
      "source": [
        "from google.colab import widgets\n",
        "\n",
        "orders = range(1,10)\n",
        "tb = widgets.TabBar([str(order) for order in orders])\n",
        "\n",
        "for order in orders:\n",
        "  with tb.output_to(str(order), select= (order < 2)):\n",
        "    \n",
        "    p = np.poly1d(np.polyfit(xTrain, yTrain, int(order)))\n",
        "\n",
        "    yPredTrain = p(xTrain)\n",
        "    error = yTrain - yPredTrain\n",
        "    errorSq = error**2\n",
        "    MSE_Train = np.sum(errorSq)/10\n",
        "\n",
        "    plt.plot(xTrain, yTrain, 'o', label='Train data')\n",
        "    plt.plot(x_LS, p(x_LS), '-', alpha=0.8, label='Learned model', linewidth=2)\n",
        "    plt.xlabel(\"x\", fontsize=14)\n",
        "    plt.ylabel(\"y\",fontsize=14)\n",
        "    plt.xlim(-1,1)\n",
        "    plt.ylim(-0.5,2.5)\n",
        "    plt.grid(alpha=0.2)\n",
        "    plt.legend(loc='upper left', fontsize=12)\n",
        "    plt.show()\n",
        "    \n",
        "    print('The coefficients of the fitted model are:\\n {} \\n\\n---\\n'.format(p.c))\n",
        "    print('The fitted model is:\\n {} \\n\\n---\\n'.format(p))\n",
        "    print('The training MSE is: {:8.3f} \\n\\n---\\n'.format(MSE_Train))\n",
        "    print(\"The SSC is: {:8.3f}\".format(sum(p.c**2)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njZPot8M0xMv"
      },
      "source": [
        "At this point, it is worth to stop and analyse the results that you have obtained. Specifically, focus on:\n",
        "\n",
        "\n",
        "* The **coefficients** of the fitted models and their **mathematical expression**.\n",
        "* The **shape** of the fitted models. Could you explain why a polynomial of degree 9 is more flexible than a polynomial of degree 1? Visually, which one makes the best prediction on the training dataset?\n",
        "*   The **training MSE**. How does it change as we increase the degree of the polynomial model? According to the training MSE value, which one makes the best prediction on the training dataset? Is it consistent with your visual interpretation?\n",
        "*   The **SSC** value. How does it change as we increase the degree of the polynomial? Have a look at the mathematical expression of the fitted models. Can you see that the **SSC** value tells us how large the coefficients are?\n",
        "\n",
        "And now the final question. **Could you tell which model is overfitting**? Try to justify very clearly why and discuss it with your classmates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-E4_Bz_jFx0"
      },
      "source": [
        "# Overfitting and validation\n",
        "\n",
        "A model is overfitting when it performs very well on the training dataset but exhibits a poor deployment performance. The polynomial of degree 9 performs really well on our training dataset. This might look suspicious to you, but until we assess a model's deployment performance, we cannot say that the model is overfitting. \n",
        "\n",
        "The training error is used for tuning a model, but never for assessing its deployment performance. **Test tasks** include a separate dataset (the test dataset) to assess the performance of a final model. **Validation tasks** are used to select a suitable family of models to be trained, and involve several rounds where we train a family of models with one dataset and assess its deployment performance with another dataset.\n",
        "\n",
        "In this section, we will implement a simple validation task to identify the right degree for our polynomial. Specifically, we will implement a **validation set** approach. After selecting the best polynomial model, we will retrain it with all the data available to produce our final model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XJDUsxyXP8d"
      },
      "source": [
        "Let's assume that the data used for training is the same that we used before and that the validation dataset is as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMYZFb2IjpNn"
      },
      "source": [
        "xVal = np.array([0.1100, 0.2300, 0.1800, 0.3500, 0.5200, -0.4400, -0.6900, -0.2400, 0.4300, -0.4100, 0.3300, 0.8800, -0.4300, 0.5600, 0.6600, -0.0100, -0.8300, 0.5700, 0.3400, 0.6700])\n",
        "yVal = np.array([1.0569, 1.0647, 0.9575, 1.2097, 0.8371, 0.8573, 0.6128, 1.1087, 0.9253, 0.9788, 1.0590, 1.6263, 0.7660, 1.0799, 1.3341, 0.6867, 0.3657, 1.1747, 1.0440, 1.1315])\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccSbewotX4zT"
      },
      "source": [
        "Let's plot both datasets on the same graph:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acfvJ_0nX-tZ"
      },
      "source": [
        "plt.plot(xTrain, yTrain, 'o', label=\"Train Data\")\n",
        "plt.plot(xVal, yVal, '*', label=\"Validation Data\")\n",
        "plt.xlabel(\"x (attribute)\", fontsize=18)\n",
        "plt.ylabel(\"y (label)\", fontsize=18)\n",
        "plt.xlim(-1,1)\n",
        "plt.ylim(-0.5,2.5)\n",
        "plt.grid(alpha=0.2)\n",
        "plt.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu2P5pJkiZfY"
      },
      "source": [
        "Visually, it is clear that our data follows a cubic pattern. Remember this is a simple dataset that we are using to illustrate key machine learning concepts. In general, we will not be able to determine the complexity of the underlying pattern so easily. \n",
        "\n",
        "Let's pretend then that we don't know what the right complexity of our model should be and use a validation set approach to select the complexity (i.e. degree) of the final polynomial model.\n",
        "\n",
        "In the following cell, we fit again polynomial models of degrees 1 to 9 and calculate their training and validation error. We also plot on the same graph the training dataset, the validation dataset and each fitted polynomial model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XRF1R_5YZlr"
      },
      "source": [
        "tbVal = widgets.TabBar([str(order) for order in orders])\n",
        "\n",
        "MSE_Train = dict.fromkeys(orders)\n",
        "MSE_Val = dict.fromkeys(orders)\n",
        "\n",
        "\n",
        "for order in orders:\n",
        "  with tbVal.output_to(str(order), select= (order < 2)):\n",
        "    \n",
        "    p = np.poly1d(np.polyfit(xTrain, yTrain, int(order)))\n",
        "\n",
        "    MSE_Train[order] = np.sum((yTrain - p(xTrain))**2)/10\n",
        "    MSE_Val[order] = np.sum((yVal - p(xVal))**2)/20\n",
        "\n",
        "\n",
        "    plt.plot(xTrain, yTrain, 'o', label='Train data')\n",
        "    plt.plot(xVal, yVal, '*', label='Validation data')\n",
        "    plt.plot(x_LS, p(x_LS), '-', alpha=0.8, label='Learned model', linewidth=2)\n",
        "    plt.xlabel(\"x\", fontsize=14)\n",
        "    plt.ylabel(\"y\",fontsize=14)\n",
        "    plt.xlim(-1,1)\n",
        "    plt.ylim(-0.5,2.5)\n",
        "    plt.grid(alpha=0.2)\n",
        "    plt.legend(loc='upper left', fontsize=12)\n",
        "    plt.show()\n",
        "    \n",
        "    print('The coefficients of the fitted model are:\\n {} \\n\\n---\\n'.format(p.c))\n",
        "    print('The fitted model is:\\n {} \\n\\n---\\n'.format(p))\n",
        "    print('The training MSE is: {:8.3f} \\n\\n---\\n'.format(MSE_Train[order]))\n",
        "    print('The validation MSE is: {:8.3f} \\n\\n---\\n'.format(MSE_Val[order]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml0YAXpbknwn"
      },
      "source": [
        "Explore all the results that you have generated and compare the figures, training MSE values and validation MSE values. It is worth paying a bit more attention to the polynomial of degree 9. Could you tell visually that its performance is excellent on the training dataset but very poor on the validation dataset?\n",
        "\n",
        "Let's plot the training and validation MSE against the order of the polynomial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "id7PZ8fRZ96U"
      },
      "source": [
        "\n",
        "plt.plot(orders, list(MSE_Train.values()), '--o', label='Training MSE', linewidth=2)\n",
        "plt.plot(orders, list(MSE_Val.values()), '--*', label='Validation MSE', linewidth=2)\n",
        "\n",
        "plt.xlabel(\"Order (degree of the fitted polynomial)\", fontsize=12)\n",
        "plt.ylabel(\"Mean Squared Error (MSE)\", fontsize=12)\n",
        "plt.xlim(1,9)\n",
        "plt.ylim(0,0.1)\n",
        "plt.grid(alpha=0.2)\n",
        "plt.legend(fontsize=14)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92bPP0xtmNjX"
      },
      "source": [
        "Have a look at the graph. The training MSE and validation MSE behave quite differently as we increes the complexity (degree) of our models. Specifically, the training MSE decreases, while the validation MSE first decreases and then increases. Can you tell which models are underfitting? Which models are overfitting? \n",
        "\n",
        "The cubic polynomial seems to be the best choice. Validation has allowed us to identify the right complexity for our polynomial model. We just used validation for **model selection**. What should our final model be? Since we have two datasets (training and validation), we can merge them together and produce a new, larger, training dataset. With this new dataset, we can train a new cubic model. This is the final cubic model that we would send for testing.\n",
        "\n",
        "Can you create a new code cell where you fit a cubic model to this new dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwpnTBshjpnS"
      },
      "source": [
        "# Regularisation\n",
        "\n",
        "When a model is fitted to a training dataset, its coefficients are allowed to take on any value. If we were to restrict the range of values that the coefficients can take on, we would be also restricting the range of shapes that the model can reproduce. In other words, we would be making the model more rigid.\n",
        "\n",
        "Regularisation is a set of techniques that add some rigidity to our models by restricting the range of values that the coefficients can take on. We will illustrate regularisation in a regression problem that uses the MSE as quality metric. In regularisation, instead of using the MSE as our quality metric **during training**, we use a modified metric which includes a regularisation term, for instance:\n",
        "\n",
        "$MSER = \\frac{1}{N}\\sum_{i=1}^N{e_i^2} + \\lambda \\sum_{k=1}^K w_k^2 = MSE + \\lambda \\times SSC$\n",
        "\n",
        "where $N$ is the number of samples, $K$ is the number of coefficients and $\\lambda$ is the regularisation parameter. Our optimisation method will then try to find a model that makes both the MSE and the SSC as small as possible. Why do we do this? Remember from previous sections that overfitting models had large SSC values. Hence, our hope is that by building models with low SSC values, we will be reducing the risk of overfitting. \n",
        "\n",
        "The regularisation parameter $\\lambda$ tells us how important it is for us to reduce the SSC value compared to the training MSE value. If $\\lambda$ is small, our optimisation method will pay more attention to reducing the MSE value. If it is large, reducing the SSC value will be more important.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBE3YUGnXH7U"
      },
      "source": [
        "In this section we will fit the polynomial model of order 9 using this regularisation approach. In the following cell, we implement ourselves the exact least squares solution with regularisation (see slide 37 of week 3's lecture notes) and obtain the regularised least squares solution for different values of lambda: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_Q6Be4pjw13"
      },
      "source": [
        "myLambdas =[0, 10**-10, 10**-9, 10**-8, 10**-7, 10**-6, 10**-5, 10**-4, 10**-3, 10**-2, 10**-1, 1, 10]\n",
        "\n",
        "tbReg = widgets.TabBar([str(myLambda) for myLambda in myLambdas])\n",
        "\n",
        "MSE_Train = dict.fromkeys(myLambdas)\n",
        "MSE_Val = dict.fromkeys(myLambdas)\n",
        "\n",
        "for myLambda in myLambdas:\n",
        "  with tbReg.output_to(str(myLambda), select= (myLambda < 2)):\n",
        "\n",
        "    XTrain = np.column_stack([np.ones(xTrain.shape), xTrain, xTrain**2, xTrain**3, xTrain**4, xTrain**5, xTrain**6, xTrain**7, xTrain**8, xTrain**9]) \n",
        "    XVal = np.column_stack([np.ones(xVal.shape), xVal, xVal**2, xVal**3, xVal**4, xVal**5, xVal**6, xVal**7, xVal**8, xVal**9])\n",
        "    w = np.dot(np.dot(np.linalg.inv(np.dot(XTrain.T, XTrain) + 10*myLambda*np.identity(10)), XTrain.T), yTrain)\n",
        "\n",
        "    X_LS = np.column_stack([np.ones(x_LS.shape), x_LS, x_LS**2, x_LS**3, x_LS**4, x_LS**5, x_LS**6, x_LS**7, x_LS**8, x_LS**9])\n",
        "    y_LS = np.dot(X_LS, w)\n",
        "\n",
        "    MSE_Train[myLambda] = np.sum((yTrain - np.dot(XTrain, w))**2)/10\n",
        "    MSE_Val[myLambda] = np.sum((yVal - np.dot(XVal, w))**2)/20\n",
        "    \n",
        "    plt.plot(xTrain, yTrain, 'o', label=\"Dataset\")\n",
        "    plt.plot(xVal, yVal, 'o', label=\"Dataset\")\n",
        "    plt.plot(x_LS, y_LS, label=\"Degree 4 prediction\")\n",
        "    plt.xlabel(\"x (attribute)\", fontsize=18)\n",
        "    plt.ylabel(\"y (label)\", fontsize=18)\n",
        "    plt.xlim(-1,1)\n",
        "    plt.ylim(-0.5,2.5)\n",
        "    plt.grid(alpha=0.2)\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.show()\n",
        "\n",
        "    print('The coefficients of the fitted model are:\\n {} \\n\\n---\\n'.format(w))\n",
        "    print('The training MSE is: {:8.3f} \\n\\n---\\n'.format(MSE_Train[myLambda]))\n",
        "    print('The validation MSE is: {:8.3f} \\n\\n---\\n'.format(MSE_Val[myLambda]))\n",
        "    print(\"The SSC is: {:8.3f}\".format(sum(w**2)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOWfSQwgle3F"
      },
      "source": [
        "Notice that as we increase the value of lambda, the complexity of the fitted model decreases. The SSC values also decrease, as optimisation will focus more on reducing the SSC values. When lambda is 10, the resulting model is practically a constant model and the SSC is close to 0. Does this result make sense to you?\n",
        "\n",
        "We can plot the training and validation values as a function of lambda. We can see that the validation MSE decreases dramatically between the values $10^{-7}$ and $10^{-2}$, indicating that we have successfully avoided overfitting within this range. Compare these results with the models plotted previously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj6MA3KHkl0c"
      },
      "source": [
        "plt.plot(myLambdas, list(MSE_Train.values()), '--o', label='Training MSE', linewidth=2, )\n",
        "plt.plot(myLambdas, list(MSE_Val.values()), '--*', label='Validation MSE', linewidth=2, )\n",
        "\n",
        "plt.xlabel(\"Lambda\", fontsize=12)\n",
        "plt.ylabel(\"Mean Squared Error (MSE)\", fontsize=12)\n",
        "plt.grid(alpha=0.2)\n",
        "plt.legend(fontsize=14)\n",
        "plt.xscale('log')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9l7KOxyniD2"
      },
      "source": [
        "# Up for a challenge?\n",
        "\n",
        "  \n",
        "Collect the training and validation datasets into a single dataset and run the following steps several times (for instance, 15 times):\n",
        "\n",
        "1.   Split **randomly** this dataset into a new training dataset containing 10 samples and a validation dataset containing 20 samples. \n",
        "2.   Fit a **linear model** and a **polynomial model of degree 9** to the training dataset, and store the resulting coefficients.\n",
        "3.   Obtain and store the training and validation **MSE** of each model.\n",
        "\n",
        "By doing this, you will have obtained 15 different linear and degree-9 polynomials. Plot all the linear models on one graph and all the degree-9 polynomial models on another. How different are the linear models to one another? And the 9-degree polynomial models? What about the MSE values, are they always the same? What is happening?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGZ9qSBPpO-c"
      },
      "source": [
        "# Summary\n",
        "\n",
        "In this lab, we have explored **overfitting** using a simple dataset. Overfitting is a key concept in machine learning and describes the situation where our models perform very well on the training dataset, but have a poor deployment performance. To assess the deployment performance, we use a dataset consisting of samples that our model has not seen before.\n",
        "\n",
        "Overfitting typically occurs in situations where our dataset is small and our model is very complex. Note that a polynomial of order 9 has 10 coefficients and our training dataset consisted of 10 samples. This can be seen as a problem with 10 unknowns (coefficients) and 10 equations (one per sample). Therefore, we could expect to find a solution capable of predicting every single sample in the training dataset.\n",
        "\n",
        "We have implemented a simple **validation approach** to select the right degree for our polynomial model. This goal is sometimes called **model selection**, and shouldn't be confused with **model training**, where we tune a family of models. Finally, we have used **regularisation** to penalise models with large coefficients. By doing this, we hope to add rigidity to our models and reduce the risk of overfitting.\n",
        "\n",
        "Remember that this is your notebook. You can explore further the examples that we have provided, make changes, improve it and be creative. You have an excellent opportunity to ask yourself \"what would happen if\" and experiment. You do not need to restrict yourself to the script.\n",
        "\n",
        "Have a go at the corresponding quiz when you feel ready."
      ]
    }
  ]
}